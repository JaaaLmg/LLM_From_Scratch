{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c21a68e",
   "metadata": {},
   "source": [
    "# GPT（124M）实现\n",
    "参考教程：https://github.com/bbruceyuan/LLMs-Zero-to-Hero\n",
    "\n",
    "\n",
    "# 1. 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c93245d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23faff1aab0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2266a7",
   "metadata": {},
   "source": [
    "# 2. 定义GPT相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f7285c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 512   # 上下文长度\n",
    "    batch_size: int = 12\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    hidden_dim:int = n_embd\n",
    "    dropout: float = 0.1\n",
    "    head_size: int = hidden_dim // n_head\n",
    "    vocab_size: int = 50257    # GPT-2的词表大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d0df7",
   "metadata": {},
   "source": [
    "# 3. 定义GPT结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ac29ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. single head attention\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.key=nn.Linear(config.hidden_dim,config.head_size)\n",
    "        self.value=nn.Linear(config.hidden_dim,config.head_size)\n",
    "        self.query=nn.Linear(config.hidden_dim,config.head_size)\n",
    "        self.head_size=config.head_size\n",
    "        \n",
    "        # attention mask 通过 register_buffer 注册在模型参数中，因为不用计算梯度，节约显存\n",
    "        self.register_buffer(\n",
    "            \"attention_mask\",\n",
    "            # 创建一个下三角矩阵\n",
    "            torch.tril(torch.ones(config.block_size,config.block_size))\n",
    "        )\n",
    "\n",
    "        self.dropout=nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size,seq_len,hidden_dim=x.size()\n",
    "        k=self.key(x)\n",
    "        v=self.value(x)\n",
    "        q=self.query(x)\n",
    "        weight=q @ k.transpose(-2,-1)\n",
    "        weight=weight.masked_fill(      # 填充下三角矩阵为负无穷\n",
    "            self.attention_mask[:seq_len,:seq_len] == 0,\n",
    "            float(\"-inf\")\n",
    "        ) / math.sqrt(self.head_size)       # 缩放\n",
    "        weight=F.softmax(weight,dim=-1)\n",
    "        weight=self.dropout(weight)     # dropout要在softmax之后\n",
    "        out=weight @ v\n",
    "        return out\n",
    "    \n",
    "# 2. multi head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                SingleHeadAttention(config) for _ in range(config.n_head)\n",
    "            ]\n",
    "        )\n",
    "        self.proj=nn.Linear(config.hidden_dim,config.hidden_dim)    # 投影\n",
    "        self.dropout=nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output=torch.cat(\n",
    "            [h(x) for h in self.heads],\n",
    "            dim=-1\n",
    "        )\n",
    "        output=self.proj(output)\n",
    "        output=self.dropout(output)\n",
    "        return output \n",
    "    \n",
    "\n",
    "# 3. feed forward\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim,4*config.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*config.hidden_dim,config.hidden_dim),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "# 4. transformer block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.att=MultiHeadAttention(config)\n",
    "        self.ffn=FeedForward(config)\n",
    "        self.ln1=nn.LayerNorm(config.hidden_dim)\n",
    "        self.ln2=nn.LayerNorm(config.hidden_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "# 5. GPT model\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)  # 词嵌入\n",
    "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)  # 位置嵌入\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.n_layer)]\n",
    "        )\n",
    "        self.ln_final = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # 现在的SLM采用tie weights，embedding和lm_head的权重共享，这样可以减少参数，加快训练\n",
    "        self.lm_head.weight = self.token_embedding_table.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 这里使用正态分布初始化\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx是输入的token id，shape为(B, T)\n",
    "        batch, seq_len = idx.size()\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "\n",
    "        # seq_len是输入的最大长度\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(seq_len, device=idx.device)    # 这里是一个长度为seq_len的数组，用于表示位置\n",
    "        )\n",
    "\n",
    "        x = token_emb + pos_emb  # 这里是把token embedding和position embedding相加\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, seq_len, vocab_size = logits.size()\n",
    "            logits = logits.view(batch*seq_len, vocab_size)\n",
    "            targets = targets.view(batch*seq_len)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # 生成新的token\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 如果输入的token长度超过block_size，则截断\n",
    "            idx_cond = idx if idx.size(1) < self.block_size else idx[:, -self.block_size:]\n",
    "            # 获取预测的token\n",
    "            logits, _ = self(idx_cond)\n",
    "            # 只关注最后一个时间步的预测结果\n",
    "            logits = logits[:, -1, :]\n",
    "            # 应用softmax获取概率分布\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # 采样下一个token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # 这里使用multinomial方法进行采样，返回的是一个索引\n",
    "            # 将预测的token添加到输入中\n",
    "            idx = torch.cat((idx,idx_next),dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f0148",
   "metadata": {},
   "source": [
    "# 4. 数据集处理\n",
    "\n",
    "- https://github.com/mobvoi/seq-monkey-data\n",
    "\n",
    "- 序列猴子数据集的下载链接如下：\n",
    "\n",
    "http://share.mobvoi.com:5000/sharing/O91blwPkY\n",
    "\n",
    "- 序列猴子数据集以 JSONL 类型文件提供。文件的每一行都是格式统一的 JSON 类型的文本。其中，JSON 的格式为：\n",
    "```text\n",
    "{\"text\": \"<文档>\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efd28294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class SeqMonkeyDataset(Dataset):\n",
    "    def __init__(self, path, block_size=512):\n",
    "        import tiktoken\n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")    # 获取GPT2的编码器\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.eos_token = self.enc.encode(\n",
    "            \"<|endoftext|>\",\n",
    "            allowed_special={\"<|endoftext|>\"}\n",
    "        )[0]\n",
    "\n",
    "        import json\n",
    "        self.encoded_data = []\n",
    "        self.max_lines = 1000   # 读取前1000行数据\n",
    "        raw_data = []\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for i,line in enumerate(f):\n",
    "                if i==0:\n",
    "                    print(json.loads(line))\n",
    "                if i>=self.max_lines:\n",
    "                    break\n",
    "                try:\n",
    "                    text = json.loads(line.strip())['text']\n",
    "                    raw_data.append(text)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        full_encoded = []\n",
    "        for text in raw_data:\n",
    "            encoded_text = self.enc.encode(text)\n",
    "            full_encoded.extend(encoded_text + [self.eos_token])\n",
    "        \n",
    "        # 将长文本分割成训练样本\n",
    "        for i in range(0, len(full_encoded), self.block_size):\n",
    "            chunck = full_encoded[i:i+self.block_size+1]    # 这里加1是因为要取target\n",
    "            # 如果chunck的长度不足block_size，则填充eos_token\n",
    "            if len(chunck) < self.block_size+1:\n",
    "                chunck = chunck + [self.eos_token]*(self.block_size+1-len(chunck))\n",
    "            self.encoded_data.append(chunck)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.encoded_data[idx]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"将文本编码为token id\"\"\"\n",
    "        return self.enc.encode(text)\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        \"\"\"将token id解码为文本\"\"\"\n",
    "        return self.enc.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab33576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '在查处虚开增值税专用发票案件中，常常涉及进项留抵税额和税款损失的认定和处理。在计算税款损失时，要不要将进项留抵税额包括在内？\\n对此，实务中存在意见分歧。\\n有人主张归并，即计算税款损失时包括进项留抵税额；\\n有人主张剥离，即计算税款损失时剔除进项留抵税额。分析这个问题，需要确定进项留抵税额与税款损失之间是什么关系。\\n理清这二者之间的关系，首先需要了解增值税的概念和其抵扣机制。增值税是以商品（货物、服务等）在流转过程中产生的增值额作为计税依据而征收的一种流转税。为避免重复征税，在增值税中存在抵扣链条机制。\\n一般而言，交易上游企业缴纳的税额，交易下游企业可以对相应的税额进行抵扣。\\n对增值税一般纳税人来说，其购进货物、服务等取得增值税专用发票，发票上的税额是进项税额。\\n其出售货物、服务等，向购买方开具增值税专用发票，发票的税额是销项税额。\\n一般情况下，销项税额减去进项税额的金额是应纳税额，企业根据应纳税额按期申报纳税。\\n其次需要了解进项留抵税额的概念及产生原因。\\n在计算销项税额和进项税额的差额时，有时会出现负数，即当期进项税额大于当期销项税额。这个差额在当期未实现抵扣，为进项留抵税额，在以后纳税人有销项税额时再进行抵扣。\\n企业产生进项留抵税额的主要原因是其进项税额和销项税额时间上的不一致。\\n例如，企业前期集中采购货物和服务，投资大，销项税率低于进项税率等。\\n从税款抵扣的角度看，进项留抵税额只是购进的这部分进项税额参与到增值税应纳税额的计算过程中，但是其对应的进项税额抵扣还未真正实现，一般要等到其未来有相应的销项税额时，才能真正实现进项税额抵扣。\\n可见，进项留抵税额处于不确定状态，能否抵扣受到很多因素影响，例如企业经营中断，没有销项税额，这时进项留抵税额就无法实现抵扣。但如果企业按照税收政策规定申请进项留抵退税，进项税额抵扣就随之实现。\\n最后需要了解税款损失的概念。\\n税款损失，通常是指因虚开增值税专用发票，导致国家税款被骗或者流失的金额。关于税款损失，实务中有多种表述。\\n例如，北京大学法学院教授陈兴良曾谈到虚开行为本身不会造成国家税款损失，只有利用发票抵扣时才会造成国家税款损失。刘兵等编著的《虚开增值税专用发票案例司法观点和案例解析》一书中提到：“给国家税款造成损失的数额，实际上就是被骗取的国家税款在侦查终结以前无法追回的部分。”\\n赵清海与王家欣合著的《增值税专用发票虚开的判定与预防》一书中提到：“司法实践中，受票方用虚开的增值税专用发票予以抵扣的税款，从而导致受票方应纳税额的减少是法院所认定的国家税款流失的金额。”\\n从这些表述可见，税款损失应该是实际造成的损失，不应包括不确定的部分——进项留抵税额，进项留抵税额与税款损失之间不能直接画等号。\\n综上分析，进项留抵税额，只是使国家税款处于可能被抵扣的状态，还没有真正造成国家税款流失，一般情况下应将其从税款损失中剥离，特殊条件下将其归并入税款损失。\\n例如，当纳税人造假按照税收政策规定申请进项留抵税额退税后，有关税款损失将会从危险状态转化成危害结果，这时候要将有关进项留抵税额并入税款损失。\\n所以，在虚开增值税专用发票案件中，一般情况下，如果以纳税人的进项税额作为税款损失的计算基数，在对其进行行政处罚或刑事处罚时，应把进项留抵税额从税款损失中剔除，但纳税人申请进项留抵退税的除外。这样处理，把处罚与危害结果相对应，体现行政处罚法的过罚相当原则和刑法的罚当其罪原则。'}\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "train_dataset = SeqMonkeyDataset('./data/mobvoi_seq_monkey_general_open_corpus.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb13aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 划分训练集和验证集\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [0.9, 0.1])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=12, shuffle=False)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "total_len = len(train_dataset)\n",
    "train_len = int(total_len * 0.9)\n",
    "val_len = total_len - train_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_len, val_len])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e158b6",
   "metadata": {},
   "source": [
    "# 5. 运行相关的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8661dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 124.046592M\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "model = GPT(GPTConfig)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# 打印一下模型共有多少参数\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params/1e6}M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b9c641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "n_epochs = 10\n",
    "output_dir = \"./checkpoints\"\n",
    "\n",
    "# # 训练循环\n",
    "# def train(model, optimizer, scheduler, train_loader, val_loader, device):\n",
    "#     model.train()   # 进入训练模式\n",
    "#     total_loss = 0\n",
    "#     for batch_idx, (x,y) in enumerate(train_loader):\n",
    "#         # 将数据移动到设备上\n",
    "#         x,y = x.to(device),y.to(device)\n",
    "#         # 前向传播\n",
    "#         logits, loss = model(x, targets=y)\n",
    "#         # 反向传播\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # 调整学习率\n",
    "#         scheduler.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {total_loss/100:.3f}\")\n",
    "#     return total_loss\n",
    "\n",
    "# def eval(model, val_loader, device):\n",
    "#     model.eval()    # 设置模型为评估模式\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for x,y in val_loader:\n",
    "#             x,y = x.to(device), y.to(device)\n",
    "#             logits, loss = model(x, targets=y)\n",
    "#             val_loss += loss.item()\n",
    "#     return val_loss\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     train_loss = train(model, optimizer, scheduler, train_loader, val_loader, device)\n",
    "#     val_loss = eval(model, val_loader, device)\n",
    "#     print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "#     # 保存模型\n",
    "#     avg_val_loss = val_loss/len(val_loader)\n",
    "#     checkpoint = {\n",
    "#         \"model_state_dict\": model.state_dict(),\n",
    "#         \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "#         \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "#         \"epoch\": epoch,\n",
    "#         \"val_loss\": avg_val_loss\n",
    "#     }\n",
    "#     # 保存每个epoch的模型\n",
    "#     torch.save(checkpoint, f\"{output_dir}/model_epoch_{epoch}.pth\")\n",
    "\n",
    "\n",
    "def train_with_logging(model, optimizer, scheduler, train_loader, val_loader, device, n_epochs=10):\n",
    "    \"\"\"\n",
    "    带日志记录的训练函数\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            logits, loss = model(x, targets=y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item():.3f}\")\n",
    "        \n",
    "        # 计算平均训练损失\n",
    "        avg_train_loss = total_loss / batch_count\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batch_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                logits, loss = model(x, targets=y)\n",
    "                val_loss += loss.item()\n",
    "                val_batch_count += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batch_count\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # 保存模型检查点\n",
    "        checkpoint = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss\n",
    "        }\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        torch.save(checkpoint, f\"{output_dir}/model_epoch_{epoch}.pth\")\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b792d5b",
   "metadata": {},
   "source": [
    "# 6. 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97835e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_loss_curves(train_losses, val_losses, epochs=None, save_path=None):\n",
    "    \"\"\"\n",
    "    绘制训练和验证损失曲线\n",
    "    \n",
    "    Args:\n",
    "        train_losses (list): 训练损失列表\n",
    "        val_losses (list): 验证损失列表  \n",
    "        epochs (int, optional): 总epoch数，如果不提供则根据列表长度自动计算\n",
    "        save_path (str, optional): 保存图片的路径\n",
    "    \"\"\"\n",
    "    if epochs is None:\n",
    "        epochs = len(train_losses)\n",
    "    \n",
    "    # 创建x轴数据\n",
    "    x_epochs = range(1, epochs + 1)\n",
    "    \n",
    "    # 设置中文字体支持\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "    plt.rcParams['axes.unicode_minus'] = False    # 用来正常显示负号\n",
    "    \n",
    "    # 创建图形\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.plot(x_epochs, train_losses, 'b-', linewidth=2, label='训练损失', marker='o')\n",
    "    plt.plot(x_epochs, val_losses, 'r-', linewidth=2, label='验证损失', marker='s')\n",
    "    \n",
    "    # 设置图表属性\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('训练和验证损失曲线')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加最小值标注\n",
    "    min_train_idx = np.argmin(train_losses)\n",
    "    min_val_idx = np.argmin(val_losses)\n",
    "    \n",
    "    plt.annotate(f'最小训练损失: {train_losses[min_train_idx]:.4f}', \n",
    "                xy=(min_train_idx + 1, train_losses[min_train_idx]),\n",
    "                xytext=(min_train_idx + 1, train_losses[min_train_idx] + 0.1),\n",
    "                arrowprops=dict(arrowstyle='->', color='blue'))\n",
    "    \n",
    "    plt.annotate(f'最小验证损失: {val_losses[min_val_idx]:.4f}',\n",
    "                xy=(min_val_idx + 1, val_losses[min_val_idx]),\n",
    "                xytext=(min_val_idx + 1, val_losses[min_val_idx] + 0.1),\n",
    "                arrowprops=dict(arrowstyle='->', color='red'))\n",
    "    \n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图片（如果指定了路径）\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"损失曲线已保存到: {save_path}\")\n",
    "    \n",
    "    # 显示图形\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(f\"\\n=== 损失统计信息 ===\")\n",
    "    print(f\"训练损失 - 最小值: {min(train_losses):.4f}, 最终值: {train_losses[-1]:.4f}\")\n",
    "    print(f\"验证损失 - 最小值: {min(val_losses):.4f}, 最终值: {val_losses[-1]:.4f}\")\n",
    "    print(f\"最佳验证损失出现在第 {np.argmin(val_losses) + 1} 个epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f91f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设您已经有了训练好的损失数据\n",
    "    train_losses, val_losses = train_with_logging(model, optimizer, scheduler, train_loader, val_loader, device)\n",
    "    \n",
    "    # 或者使用已有的损失数据绘图\n",
    "    # 示例数据\n",
    "    # sample_train_losses = [2.5, 2.1, 1.8, 1.6, 1.4, 1.3, 1.2, 1.1, 1.05, 1.0]\n",
    "    # sample_val_losses = [2.4, 2.0, 1.7, 1.5, 1.4, 1.35, 1.3, 1.25, 1.2, 1.15]\n",
    "    \n",
    "    # 绘制曲线\n",
    "    plot_loss_curves(train_losses, val_losses, save_path=\"./loss_curve.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
